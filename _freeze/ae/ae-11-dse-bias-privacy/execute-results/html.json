{
  "hash": "b94a970b7f181881092c5680298c7748",
  "result": {
    "markdown": "---\ntitle: \"AE 11: Data science ethics\"\nsubtitle: \"Suggested answers\"\ncategories: \n  - Application exercise\n  - Answers\neditor: visual\n---\n\n\n# Part 1 - Data privacy \n\nConsider the following scenario: There appears to be an increase in bicycle accidents around the school before and after class. You have been tasked with collecting data to help protect the health of your peers and improve your community. What data might you collect and how? What responsibility do you have to protect that\ndata?\n\nSpecifically: \n\n- which data would you collect\n\n*Class Answers* - Location, weather, severity \n\n- how would you collect the data\n\n*Class Answers* - Traffic cameras, hospital reports, weather reports\n\n- how would you keep data private. \n\n*Class Answers* - Not collect personal information, de-identify information \n\n\n\n# Part 2 - Predicting ethnicity - data ethics \n\n**Your turn (12 minutes):** Imai and Khanna (2016) built a racial prediction algorithm using a Bayes classifier trained on voter registration records from Florida and the U.S. Census Bureau's name list.\n\n-   The following is the title and the abstract of the paper. Take a minute to read them.\n\n> [Improving Ecological Inference by Predicting Individual Ethnicity from Voter Registration Record](https://imai.fas.harvard.edu/research/race.html) (Imran and Khan, 2016)\n\n> In both political behavior research and voting rights litigation, turnout and vote choice for different racial groups are often inferred using aggregate election results and racial composition. Over the past several decades, many statistical methods have been proposed to address this ecological inference problem. We propose an alternative method to reduce aggregation bias by predicting individual-level ethnicity from voter registration records. Building on the existing methodological literature, we use Bayes's rule to combine the Census Bureau's Surname List with various information from geocoded voter registration records. We evaluate the performance of the proposed methodology using approximately nine million voter registration records from Florida, where self-reported ethnicity is available. We find that it is possible to reduce the false positive rate among Black and Latino voters to 6% and 3%, respectively, while maintaining the true positive rate above 80%. Moreover, we use our predictions to estimate turnout by race and find that our estimates yields substantially less amounts of bias and root mean squared error than standard ecological inference estimates. We provide open-source software to implement the proposed methodology. The open-source software is available for implementing the proposed methodology.\n\nThe said \"source software\" is the **wru** package: <https://github.com/kosukeimai/wru>.\n\n-   Then, if you feel comfortable, install the **wru** package and try it out using the sample data provided in the package. And if you don't feel comfortable doing so, take a look at the results below. Was the publication of this model ethical? Does the open-source nature of the code affect your answer? Is it ethical to use this software? Does your answer change depending on the intended use?\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# install.packages(\"wru\")\n\nlibrary(tidyverse)\nlibrary(wru)\n\npredict_race(voter.file = voters, surname.only = TRUE) %>% \n  select(surname, pred.whi, pred.bla, pred.his, pred.asi, pred.oth)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n      surname     pred.whi     pred.bla     pred.his   pred.asi   pred.oth\n1      Khanna 0.0049265455 0.0016079483 0.0023108109 0.88994257 0.10121213\n2        Imai 0.0059040605 0.0007184811 0.0193291824 0.76407641 0.20997186\n3      Rivera 0.0133185684 0.0121164643 0.8680470387 0.07086170 0.03565623\n4     Fifield 0.5122939613 0.0052487193 0.0596200858 0.06003480 0.36280243\n5        Zhou 0.0006977652 0.0006618759 0.0001766502 0.98773098 0.01073272\n6    Ratkovic 0.3845625798 0.0176011903 0.0131533900 0.04793387 0.53674897\n7     Johnson 0.1700395443 0.5164452995 0.0263012602 0.02193102 0.26528287\n8       Lopez 0.0122148735 0.0074083470 0.9026491278 0.03610843 0.04161922\n10 Wantchekon 0.1016771922 0.2873197891 0.3838356390 0.06225610 0.16491128\n9       Morse 0.4688011703 0.1153905281 0.0411994830 0.05237443 0.32223438\n```\n:::\n:::\n\n\n-   If you have installed the package, re-run the code, this time to see what the package predicts for your race. Now consider the same questions again: Was the publication of this model ethical? Does the open-source nature of the code affect your answer? Is it ethical to use this software? Does your answer change depending on the intended use?\n\n*Answers will vary*\n\n*Class answers* - It depends on the usage\n\n\n::: {.cell}\n\n```{.r .cell-code}\nme <- tibble(surname = \"Blake\")\n\npredict_race(voter.file = me, surname.only = TRUE)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Unknown or uninitialised column: `state`.\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nProceeding with last name predictions...\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nâ„¹ All local files already up-to-date!\nâ„¹ All local files already up-to-date!\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n  surname  pred.whi  pred.bla   pred.his    pred.asi   pred.oth\n1   Blake 0.6663199 0.2276731 0.03137626 0.007787957 0.06684285\n```\n:::\n:::\n\n\n# Part 3 - Bias \n\n![](images/getty.png){fig-align=\"center\" width=\"700\"}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\ndata <- tibble(x = c(3,4,5,4,3,2,3,4,7) )\n\ndata |>\n  ggplot() + \n  geom_histogram(aes(x = x), bins = 20) + \n  geom_vline(xintercept = 4.22 , colour = \"red\" , size = 2) +\n  geom_vline(xintercept = mean(data$x) , color = \"blue\" , size = 2)\n```\n\n::: {.cell-output-display}\n![](ae-11-dse-bias-privacy_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\n- What are your major takeaways from this activity?\n\n- How does this concept relate to bias in algorithms? \n\n\n## Optional \n\nPart 4 - Stochastic parrots\n\n**Your turn (10 minutes):**\n\n-   Read the following title and abstract.\n\n> [On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? ðŸ¦œ](https://s10251.pcdn.co/pdf/2021-bender-parrots.pdf) (Bender et. al., 2021)\n>\n> The past 3 years of work in NLP have been characterized by the development and deployment of ever larger language models, especially for English. BERT, its variants, GPT-2/3, and others, most recently Switch-C, have pushed the boundaries of the possible both through architectural innovations and through sheer size. Using these pretrained models and the methodology of fine-tuning them for specific tasks, researchers have extended the state of the art on a wide array of tasks as measured by leaderboards on specific benchmarks for English. In this paper, we take a step back and ask: How big is too big? What are the possible risks associated with this technology and what paths are available for mitigating those risks? We provide recommendations including weighing the environmental and financial costs first, investing resources into curating and carefully documenting datasets rather than ingesting everything on the web, carrying out pre-development exercises evaluating how the planned approach fits into research and development goals and supports stakeholder values, and encouraging research directions beyond ever larger language models.\n\n-   Have you used a natural language model before? Describe your use.\n\n-   What is meant by \"stochastic parrots\" in the paper title?\n",
    "supporting": [
      "ae-11-dse-bias-privacy_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}