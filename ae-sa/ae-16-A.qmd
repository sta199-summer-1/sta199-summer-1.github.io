---
title: "Suggested Answers: Prediction"
format: html
editor: visual
---

```{r}
#| label: load-packages
#| message: false
library(tidyverse)
library(tidymodels)
library(ggridges)
library(patchwork)
library(palmerpenguins)
```

By the end of today, you will...

-   Understand the ideas behind testing vs training data sets
-   (continued) fit a logistic regression model in R
-   Evaluate your model based on predictions
-   Evaluate linear regression model + check assumption

We will again be working with the email data set. Please re-familiarize yourself with these data below:

```{r}
email <- read_csv("data/email.csv") |>
  mutate(spam = factor(spam),
         image = factor(image))
```

```{r}
#|label: glimpse

glimpse(email)
```

## Testing vs Training

Before we decide on our model, let's build a testing and training data set using the following code.

-- Go through line by line and comment what the code is doing...

Once complete, take a glimpse at each new data set.

```{r}
set.seed(0310) # to make sure we are consist

email_split <- initial_split(email, prop = 0.80) 

train_data <- training(email_split)
test_data <- testing(email_split)

glimpse(test_data)
```

Now that our testing and training data sets have been built, let's practice picking a model!

First, we are going to decide on our model to model the if an email is spam or not.

The variables we'll use for the first model will be:

-   `spam`: 1 if the email is spam, 0 otherwise
-   `exclaim_mess`: The number of exclamation points in the email message
-   `winner`: Has the word "winner" in the email or not

The variables we'll use for the second model will be:

-   `spam`: 1 if the email is spam, 0 otherwise
-   `exclaim_mess`: The number of exclamation points in the email message
-   `image`: Had an image attached to the email

Now, fit each of these two additive generalized linear models below. Use AIC to make an argument for which model we should use to analyze if an email is spam or not.

```{r}

email_fit <- logistic_reg() |>
  set_engine("glm") |>
  fit(spam ~ exclaim_mess + winner, data = train_data, family = "binomial")

glance(email_fit)$AIC  
  
model2 <- logistic_reg() |>
  set_engine("glm") |>
  fit(spam ~ exclaim_mess + image, data = train_data , family = "binomial")

glance(model2)$AIC

```

Name your chosen model `email_fit`.

## Prediction

Now, let's evaluate our model using our test data using the following code below. Comment on what the code is doing...

Note: Not predicting probability of success for a single response (type = response - see ae-15); Calculating probabilities for both success and failure for all of testing data (type = "prob)

```{r}
email_pred <- predict(email_fit, test_data, type = "prob") |>  
  bind_cols(test_data |> select(spam))  
```

Name the above tibble `email_pred`

## How can we plot this?

Make an Receiver operating characteristic (ROC) curve (plot true positive rate vs false positive rate)

```{r}
email_pred |>
  roc_curve(
    truth = spam, 
    .pred_1, 
    event_level = "second" #which level is a success?
  ) |>
  autoplot()
```

**Note** Any small movement to the right indicates when a decision was made incorrectly.

What is this ROC curve telling us?

We have more true positives than false positives. We are doing a better job than just predicting spam by flipping a coin (50-50).

How did R create this graph?

**Picked a cutoff that maximizes area under the ROC curve**

## Area under the curve

We can calculate the area under the curve using the following code:

```{r}

email_pred |>
  roc_auc(
    truth = spam, 
    .pred_1, 
    event_level = "second" #which level is a success
  ) 

```

What is the AUC?

**0.650**

There are two things we can do with this number...

-- Is this number \> 0.5?

-- How does this number compare to another AUC calculation?

## Your Turn!

-- Fit a competing model

-- Generate an ROC plot

-- Calculate the AUC and compare it to the model above

Hint: You can either rewrite code from above, or copy, paste, and edit to save time during class

```{r}

model_class <- logistic_reg() |>
  set_engine("glm") |>
    fit(spam ~ viagra + exclaim_mess , data = train_data , family = "binomial")

glance(model_class)$AIC

email_pred2 <- predict(model_class, test_data, type = "prob") |>  
  bind_cols(test_data |> select(spam))  


email_pred2 |>
  roc_curve(
    truth = spam, 
    .pred_1, 
    event_level = "second" #which level is a success?
  ) |>
  autoplot()

email_pred2 |>
  roc_auc(
    truth = spam, 
    .pred_1, 
    event_level = "second" #which level is a success
  ) 

```

## Questions for next time?

In general, how does a cut point effect sensitivity (true positive)? False negatives?

We will explore this in R next time!

# Only if time

# Linear Regression

## What if we don't have a testing data set?

![](images/penguin.png){fig-align="center"}

These are the data our model were trained on. Not optimal for assessing performance but it is something.

Even if we don't have a test data set, we could still create a new column of predictions like before:

Context of Penguins data set

```{r}
# predict based on new data

myPredictiveModel <- linear_reg() |>
  set_engine("lm") |>
  fit(body_mass_g ~ flipper_length_mm, data = penguins)

predict_peng <- penguins |>
  mutate(myPrediction = predict(myPredictiveModel, penguins)$.pred)

```

From here we can plot $\hat{y}$ vs $y$:

```{r}
predict_peng |>
  ggplot(aes(x = body_mass_g, y = myPrediction)) +
  geom_point() +
  labs(x = "True Body Mass", y = "Predicted Body Mass", title = "Predicted vs True Body Mass") +
  geom_abline(slope = 1, intercept = 0, color = "steelblue")
```

## Assumptions of Linear Regression

Alternatively, we could create a **residual plot**. Residual plots can be used to assess whether a linear model is appropriate.

A common assumption of linear regression models is that the error term, $\epsilon$, has constant variance everywhere.

-   If the linear model is appropriate, a residual plot should show this.

-   Patterned or nonconstant residual spread may sometimes be indicative a model is missing predictors or missing interactions.

## Residuals

Create a new column `residuals` in `predict_peng` and save your data frame as `predict_peng_2`


## The Plot

```{r}

predict_peng_2 <- predict_peng |>
  mutate(residuals = body_mass_g - myPrediction)


predict_peng_2 |>
  ggplot(aes(x = myPrediction, y = residuals)) + 
  geom_point() +
  geom_hline(yintercept = 0) +
  labs(x = "Predicted body_mass", y = "Residual")
```



